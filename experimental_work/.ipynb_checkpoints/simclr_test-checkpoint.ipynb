{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cardiovascular-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1+cu101\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import json, os, sys, random, pickle\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms \n",
    "import os\n",
    "import skimage\n",
    "#import IPython.display\n",
    "#import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import urllib\n",
    "from collections import OrderedDict\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms \n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomCrop\n",
    "import skimage\n",
    "import IPython.display\n",
    "import urllib\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import clip\n",
    "#from skimage import io\n",
    "from pycocotools.coco import COCO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import argparse\n",
    "from model_init import *\n",
    "from data_loader import *\n",
    "from cosine_exp import *\n",
    "from models_def.pytorch_models import *\n",
    "from models_def.clip_model import *\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "correct-magnet",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4076a9ceb8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLARS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[1;32m      9\u001b[0m     \u001b[0mLayer\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwise\u001b[0m \u001b[0madaptive\u001b[0m \u001b[0mrate\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import gc\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer-wise adaptive rate scaling\n",
    "    - Converted from Tensorflow to Pytorch from:\n",
    "    https://github.com/google-research/simclr/blob/master/lars_optimizer.py\n",
    "    - Based on:\n",
    "    https://github.com/noahgolmant/pytorch-lars\n",
    "    params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): base learning rate (\\gamma_0)\n",
    "        lr (int): Length / Number of layers we want to apply weight decay, else do not compute\n",
    "        momentum (float, optional): momentum factor (default: 0.9)\n",
    "        use_nesterov (bool, optional): flag to use nesterov momentum (default: False)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0.0)\n",
    "            (\"\\beta\")\n",
    "        eta (float, optional): LARS coefficient (default: 0.001)\n",
    "    - Based on Algorithm 1 of the following paper by You, Gitman, and Ginsburg.\n",
    "    - Large Batch Training of Convolutional Networks:\n",
    "        https://arxiv.org/abs/1708.03888\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, len_reduced, momentum=0.9, use_nesterov=False, weight_decay=0.0, classic_momentum=True, eta=0.001):\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eta=eta,\n",
    "            len_reduced=len_reduced\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eta = eta\n",
    "        self.len_reduced = len_reduced\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            learning_rate = group['lr']\n",
    "\n",
    "            # TODO: Hacky\n",
    "            counter = 0\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: This really hacky way needs to be improved.\n",
    "                # Note Excluded are passed at the end of the list to are ignored\n",
    "                if counter < self.len_reduced:\n",
    "                    grad += self.weight_decay * param\n",
    "\n",
    "                # Create parameter for the momentum\n",
    "                if \"momentum_var\" not in param_state:\n",
    "                    next_v = param_state[\"momentum_var\"] = torch.zeros_like(\n",
    "                        p.data\n",
    "                    )\n",
    "                else:\n",
    "                    next_v = param_state[\"momentum_var\"]\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: implementation of layer adaptation\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "\n",
    "                    trust_ratio = torch.where(w_norm.ge(0), torch.where(\n",
    "                        g_norm.ge(0), (self.eta * w_norm / g_norm), torch.Tensor([1.0]).to(device)), torch.Tensor([1.0]).to(device)).item()\n",
    "\n",
    "                    scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "                    next_v.mul_(momentum).add_(scaled_lr, grad)\n",
    "\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "\n",
    "                # Not classic_momentum\n",
    "                else:\n",
    "\n",
    "                    next_v.mul_(momentum).add_(grad)\n",
    "\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (grad)\n",
    "\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: implementation of layer adaptation\n",
    "                    w_norm = torch.norm(param)\n",
    "                    v_norm = torch.norm(update)\n",
    "\n",
    "                    device = v_norm.get_device()\n",
    "\n",
    "                    trust_ratio = torch.where(w_norm.ge(0), torch.where(\n",
    "                        v_norm.ge(0), (self.eta * w_norm / v_norm), torch.Tensor([1.0]).to(device)), torch.Tensor([1.0]).to(device)).item()\n",
    "\n",
    "                    scaled_lr = learning_rate * trust_ratio\n",
    "\n",
    "                    p.data.add_(-scaled_lr * update)\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "def get_optimiser(models, mode):\n",
    "    '''Get the desired optimiser\n",
    "    - Selects and initialises an optimiser with model params.\n",
    "    - if 'LARS' is selected, the 'bn' and 'bias' parameters are removed from\n",
    "     model optimisation, only passing the parameters we want.\n",
    "    Args:\n",
    "        models (tuple): models which we want to optmise, (e.g. encoder and projection head)\n",
    "        mode (string): the mode of training, (i.e. 'pretrain', 'finetune')\n",
    "        args (Dictionary): Program Arguments\n",
    "    Returns:\n",
    "        optimiser (torch.optim.optimizer):\n",
    "    '''\n",
    "\n",
    "    # Learning Rate\n",
    "    #args.scaled_learning_rate = (args.learning_rate * (args.batch_size / 256))\n",
    "    #args.scaled_finetune_learning_rate = (args.finetune_learning_rate * (args.batch_size / 256))\n",
    "    scaled_learning_rate = (0.03 * (265 / 256))\n",
    "    scaled_finetune_learning_rate = (0.0003 * (256 / 256))\n",
    "\n",
    "    params_models = []\n",
    "    reduced_params = []\n",
    "\n",
    "    removed_params = []\n",
    "\n",
    "    skip_lists = ['bn', 'bias']\n",
    "\n",
    "    for m in models:\n",
    "\n",
    "        m_skip = []\n",
    "        m_noskip = []\n",
    "\n",
    "        params_models += list(m.parameters())\n",
    "\n",
    "        for name, param in m.named_parameters():\n",
    "            if (any(skip_name in name for skip_name in skip_lists)):\n",
    "                m_skip.append(param)\n",
    "            else:\n",
    "                m_noskip.append(param)\n",
    "        reduced_params += list(m_noskip)\n",
    "        removed_params += list(m_skip)\n",
    "    # Set hyperparams depending on mode\n",
    "    if mode == 'pretrain':\n",
    "        lr = args.scaled_learning_rate\n",
    "        wd = args.weight_decay\n",
    "        opt = args.optimiser\n",
    "    else:\n",
    "        lr = scaled_finetune_learning_rate\n",
    "        #wd = args.finetune_weight_decay\n",
    "        wd = 1e-5\n",
    "        opt = 'lars'\n",
    "\n",
    "    # Select Optimiser\n",
    "    if opt == 'adam':\n",
    "\n",
    "        optimiser = optim.Adam(params_models, lr=lr,\n",
    "                               weight_decay=wd)\n",
    "\n",
    "    elif opt == 'sgd':\n",
    "\n",
    "        optimiser = optim.SGD(params_models, lr=lr,\n",
    "                              weight_decay=wd, momentum=0.9, nesterov=True)\n",
    "\n",
    "    elif opt == 'lars':\n",
    "\n",
    "        print(\"reduced_params len: {}\".format(len(reduced_params)))\n",
    "        print(\"removed_params len: {}\".format(len(removed_params)))\n",
    "\n",
    "        optimiser = LARS(reduced_params+removed_params, lr=lr,\n",
    "                         weight_decay=wd, eta=0.001, use_nesterov=True, len_reduced=len(reduced_params))\n",
    "    else:\n",
    "\n",
    "        raise NotImplementedError('{} not setup.'.format(args.optimiser))\n",
    "\n",
    "    return optimiser\n",
    "\n",
    "def finetune(encoder, mlp, dataloaders):\n",
    "    ''' Finetune script - SimCLR\n",
    "        Freeze the encoder and train the supervised classification head with a Cross Entropy Loss.\n",
    "    '''\n",
    "\n",
    "    mode = 'finetune'\n",
    "\n",
    "    ''' Optimisers '''\n",
    "    # Only optimise the supervised head\n",
    "    optimiser = get_optimiser((mlp,), mode)\n",
    "\n",
    "    ''' Schedulers '''\n",
    "    # Cosine LR Decay\n",
    "    lr_decay = lr_scheduler.CosineAnnealingLR(optimiser, 10)\n",
    "\n",
    "    ''' Loss / Criterion '''\n",
    "    #criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "    # initilize Variables\n",
    "    best_valid_loss = np.inf\n",
    "    best_valid_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    ''' Pretrain loop '''\n",
    "    for epoch in range(10):\n",
    "        res = list()\n",
    "\n",
    "        # Freeze the encoder, train classification head\n",
    "        encoder.eval()\n",
    "        mlp.train()\n",
    "\n",
    "        sample_count = 0\n",
    "        run_loss = 0\n",
    "        run_top1 = 0.0\n",
    "        run_top5 = 0.0\n",
    "\n",
    "        # Print setup for distributed only printing on one node.\n",
    "        #if args.print_progress:\n",
    "            #logging.info('\\nEpoch {}:\\n'.format(epoch+1))\n",
    "            # tqdm for process (rank) 0 only when using distributed training\n",
    "        train_dataloader = tqdm.tqdm(dataloaders['train'])\n",
    "        #else:\n",
    "            #train_dataloader = dataloaders['train']\n",
    "\n",
    "        ''' epoch loop '''\n",
    "        for i, (inputs, target) in enumerate(train_dataloader):\n",
    "\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "            # Forward pass\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            # Do not compute the gradients for the frozen encoder\n",
    "            with torch.no_grad():\n",
    "                h = encoder(inputs)\n",
    "\n",
    "            # Take pretrained encoder representations\n",
    "            output = mlp(h)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            #print(\"LOSS: \", loss)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimiser.step()\n",
    "            \n",
    "            preds = torch.sigmoid(output)\n",
    "            res.append((preds.data.cpu(), target.data.cpu()))\n",
    "\n",
    "\n",
    "            sample_count += inputs.size(0)\n",
    "\n",
    "            run_loss += loss.item()\n",
    "\n",
    "            #predicted = output.argmax(1)\n",
    "\n",
    "            #acc = (predicted == target).sum().item() / target.size(0)\n",
    "\n",
    "            #run_top1 += acc\n",
    "\n",
    "            #_, output_topk = output.topk(5, 1, True, True)\n",
    "\n",
    "           # acc_top5 = (output_topk == target.view(-1, 1).expand_as(output_topk)\n",
    "                        #).sum().item() / target.size(0)  # num corrects\n",
    "\n",
    "            #run_top5 += acc_top5\n",
    "\n",
    "        epoch_finetune_loss = run_loss / len(dataloaders['train'])  # sample_count\n",
    "        total_preds = torch.cat([entry[0] for entry in res], 0) \n",
    "        total_targets = torch.cat([entry[1] for entry in res], 0) \n",
    "        meanAP = average_precision_score(total_targets.numpy(), total_preds.numpy(), average='macro')\n",
    "        print(\"MEAN AVG PRECISION: \", meanAP)\n",
    "        print(\"EPOCH LOSS: \", epoch_finetune_loss)\n",
    "\n",
    "\n",
    "        #epoch_finetune_acc = run_top1 / len(dataloaders['train'])\n",
    "\n",
    "        #epoch_finetune_acc_top5 = run_top5 / len(dataloaders['train'])\n",
    "\n",
    "        ''' Update Schedulers '''\n",
    "        # Decay lr with CosineAnnealingLR\n",
    "        lr_decay.step()\n",
    "\n",
    "        ''' Printing '''\n",
    "        #if args.print_progress:  # only validate using process 0\n",
    "            #print('\\n[Finetune] loss: {:.4f},\\t acc: {:.4f}, \\t acc_top5: {:.4f}\\n'.format(\n",
    "                #epoch_finetune_loss, epoch_finetune_acc, epoch_finetune_acc_top5))\n",
    "\n",
    "            #args.writer.add_scalars('finetune_epoch_loss', {'train': epoch_finetune_loss}, epoch+1)\n",
    "            #args.writer.add_scalars('finetune_epoch_acc', {'train': epoch_finetune_acc}, epoch+1)\n",
    "            #args.writer.add_scalars('finetune_epoch_acc_top5', {\n",
    "                                    #'train': epoch_finetune_acc_top5}, epoch+1)\n",
    "            #args.writer.add_scalars(\n",
    "                #'finetune_lr', {'train': optimiser.param_groups[0]['lr']}, epoch+1)\n",
    "\n",
    "        #valid_loss, valid_acc, valid_acc_top5 = evaluate(\n",
    "            #encoder, mlp, dataloaders, 'val', epoch)\n",
    "\n",
    "        # For the best performing epoch, reset patience and save model,\n",
    "        # else update patience.\n",
    "        #if valid_acc >= best_valid_acc:\n",
    "            #patience_counter = 0\n",
    "            #best_epoch = epoch + 1\n",
    "            #best_valid_acc = valid_acc\n",
    "\n",
    "            # saving using process (rank) 0 only as all processes are in sync\n",
    "\n",
    "        state = {\n",
    "            #'args': args,\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'supp_mlp': mlp.state_dict(),\n",
    "            'optimiser': optimiser.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "\n",
    "            #torch.save(state, (args.checkpoint_dir[:-3] + \"_finetune.pt\"))\n",
    "\n",
    "        epoch_finetune_loss = None  # reset loss\n",
    "        epoch_finetune_acc = None\n",
    "        epoch_finetune_acc_top5 = None\n",
    "\n",
    "    del state\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()  # release unreferenced memory\n",
    "\n",
    "\n",
    "def evaluate(encoder, mlp, dataloaders, mode, epoch):\n",
    "    ''' Evaluate script - SimCLR\n",
    "        evaluate the encoder and classification head with Cross Entropy loss.\n",
    "    '''\n",
    "\n",
    "    epoch_valid_loss = None  # reset loss\n",
    "    epoch_valid_acc = None  # reset acc\n",
    "    epoch_valid_acc_top5 = None\n",
    "\n",
    "    ''' Loss / Criterion '''\n",
    "    #criterion = nn.CrossEntropyLoss().cuda()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "    # initilize Variables\n",
    "    #args.writer = SummaryWriter(args.summaries_dir)\n",
    "\n",
    "    # Evaluate both encoder and class head\n",
    "    encoder.eval()\n",
    "    mlp.eval()\n",
    "\n",
    "    # initilize Variables\n",
    "    sample_count = 0\n",
    "    run_loss = 0\n",
    "    run_top1 = 0.0\n",
    "    run_top5 = 0.0\n",
    "\n",
    "    # Print setup for distributed only printing on one node.\n",
    "    #if args.print_progress:\n",
    "            # tqdm for process (rank) 0 only when using distributed training\n",
    "    eval_dataloader = tqdm.tqdm(dataloaders[mode])\n",
    "    #else:\n",
    "        #eval_dataloader = dataloaders[mode]\n",
    "    res = list()\n",
    "    ''' epoch loop '''\n",
    "    for i, (inputs, target) in enumerate(eval_dataloader):\n",
    "\n",
    "        # Do not compute gradient for encoder and classification head\n",
    "        encoder.zero_grad()\n",
    "        mlp.zero_grad()\n",
    "\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "\n",
    "        h = encoder(inputs)\n",
    "\n",
    "        output = mlp(h)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        print(\"LOSS: \", loss)\n",
    "\n",
    "        #torch.cuda.synchronize()\n",
    "\n",
    "        sample_count += inputs.size(0)\n",
    "\n",
    "        run_loss += loss.item()\n",
    "        \n",
    "        preds = torch.sigmoid(output)\n",
    "        res.append((preds.data.cpu(), target.data.cpu()))\n",
    "\n",
    "        #predicted = output.argmax(-1)\n",
    "\n",
    "        #acc = (predicted == target).sum().item() / target.size(0)\n",
    "\n",
    "        #run_top1 += acc\n",
    "\n",
    "        #_, output_topk = output.topk(5, 1, True, True)\n",
    "\n",
    "        #acc_top5 = (output_topk == target.view(-1, 1).expand_as(output_topk)\n",
    "                    #).sum().item() / target.size(0)  # num corrects\n",
    "\n",
    "        #run_top5 += acc_top5\n",
    "        \n",
    "        \n",
    "\n",
    "    epoch_valid_loss = run_loss / len(dataloaders[mode])  # sample_count\n",
    "    total_preds = torch.cat([entry[0] for entry in res], 0) \n",
    "    total_targets = torch.cat([entry[1] for entry in res], 0)\n",
    "    print(\"EPOCH LOSS: \", epoch_valid_loss)\n",
    "    meanAP = average_precision_score(total_targets.numpy(), total_preds.numpy(), average='macro')\n",
    "    print(\"MEAN AVG PRECISION: \", meanAP)\n",
    "\n",
    "    #epoch_valid_acc = run_top1 / len(dataloaders[mode])\n",
    "\n",
    "    #epoch_valid_acc_top5 = run_top5 / len(dataloaders[mode])\n",
    "\n",
    "    ''' Printing '''\n",
    "    #if args.print_progress:  # only validate using process 0\n",
    "        #print('\\n[{}] loss: {:.4f},\\t acc: {:.4f},\\t acc_top5: {:.4f} \\n'.format(\n",
    "            #mode, epoch_valid_loss, epoch_valid_acc, epoch_valid_acc_top5))\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    gc.collect()  # release unreferenced memory\n",
    "\n",
    "    return epoch_valid_loss, epoch_valid_acc, epoch_valid_acc_top5\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "decimal-fault",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-31-d018916be0a9>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-d018916be0a9>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    '''Resnet Class\u001b[0m\n\u001b[0m                   \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2', 'projection_MLP', 'Sup_Head']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "'''Resnet Class\n",
    "    Taken from:\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "'''\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers,   args, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "\n",
    "        # Different model for smaller image size\n",
    "        if args.dataset == 'cifar10' or args.dataset == 'cifar100' or args.dataset == 'tinyimagenet':\n",
    "\n",
    "            # CIFAR Stem\n",
    "\n",
    "            self.stem = nn.Sequential()\n",
    "\n",
    "            self.stem.add_module('conv0', nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1,\n",
    "                                                    bias=False))\n",
    "            self.stem.add_module('BN1', norm_layer(self.inplanes))\n",
    "            self.stem.add_module('ReLU1', nn.ReLU(inplace=True))\n",
    "\n",
    "        # e.g. ImageNet\n",
    "        else:\n",
    "\n",
    "            self.stem = nn.Sequential()\n",
    "\n",
    "            self.stem.add_module('conv0', nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                                                    bias=False))\n",
    "            self.stem.add_module('BN1', norm_layer(self.inplanes))\n",
    "            self.stem.add_module('ReLU1', nn.ReLU(inplace=True))\n",
    "            self.stem.add_module('MaxPool1', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(args,   **kwargs):\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        args: arguments\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(args,   **kwargs):\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        args: arguments\n",
    "    \"\"\"\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(args,   **kwargs):\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        args: arguments\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(args,   **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        args: arguments\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(args,   **kwargs):\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        args: arguments\n",
    "    \"\"\"\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],   args, **kwargs)\n",
    "\n",
    "\n",
    "''' SimCLR Projection and Classification Heads '''\n",
    "\n",
    "\n",
    "class Sup_Head(nn.Module):\n",
    "    '''Supervised Classification head for the finetuning of the resnet encoder.\n",
    "        - Uses the dataset and model size to determine encoder output\n",
    "            representation dimension.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sup_Head, self).__init__()\n",
    "\n",
    "        #if args.model == 'resnet18' or args.model == 'resnet34':\n",
    "           # n_channels = 512\n",
    "        #elif args.model == 'resnet50' or args.model == 'resnet101' or args.model == 'resnet152':\n",
    "        n_channels = 2048\n",
    "        #else:\n",
    "            #raise NotImplementedError('model not supported: {}'.format(args.model))\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "\n",
    "        self.classifier.add_module('W1', nn.Linear(\n",
    "            n_channels, 80))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class projection_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''Projection head for the pretraining of the resnet encoder.\n",
    "            - Uses the dataset and model size to determine encoder output\n",
    "                representation dimension.\n",
    "            - Outputs to a dimension of 128, and uses non-linear activation\n",
    "                as described in SimCLR paper: https://arxiv.org/pdf/2002.05709.pdf\n",
    "        '''\n",
    "        super(projection_MLP, self).__init__()\n",
    "\n",
    "        #if args.model == 'resnet18' or args.model == 'resnet34':\n",
    "            #n_channels = 512\n",
    "        #elif args.model == 'resnet50' or args.model == 'resnet101' or args.model == 'resnet152':\n",
    "        n_channels = 2048\n",
    "        #else:\n",
    "            #raise NotImplementedError('model not supported: {}'.format(args.model))\n",
    "\n",
    "        self.projection_head = nn.Sequential()\n",
    "\n",
    "        self.projection_head.add_module('W1', nn.Linear(\n",
    "            n_channels, n_channels))\n",
    "        self.projection_head.add_module('BN1', nn.BatchNorm1d(n_channels))\n",
    "        self.projection_head.add_module('ReLU', nn.ReLU())\n",
    "        self.projection_head.add_module('W2', nn.Linear(\n",
    "            n_channels, 128))\n",
    "        self.projection_head.add_module('BN2', nn.BatchNorm1d(128))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2', 'projection_MLP', 'Sup_Head']\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "'''Resnet Class\n",
    "    Taken from:\n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.downsample = downsample  # hack: moving downsample to the first to make order correct\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None, width_mult=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64 * width_mult\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64 * width_mult, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128 * width_mult, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256 * width_mult, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512 * width_mult, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion * width_mult, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "    \n",
    "def resnet50x1(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs, width_mult=1)\n",
    "\n",
    "def resnet50x2(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], width_mult=2)\n",
    "\n",
    "\n",
    "def resnet50x4(**kwargs):\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], width_mult=4)\n",
    "\n",
    "''' SimCLR Projection and Classification Heads '''\n",
    "\n",
    "\n",
    "class Sup_Head(nn.Module):\n",
    "    '''Supervised Classification head for the finetuning of the resnet encoder.\n",
    "        - Uses the dataset and model size to determine encoder output\n",
    "            representation dimension.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super(Sup_Head, self).__init__()\n",
    "\n",
    "        #if args.model == 'resnet18' or args.model == 'resnet34':\n",
    "        #   n_channels = 512\n",
    "        #elif args.model == 'resnet50' or args.model == 'resnet101' or args.model == 'resnet152':\n",
    "        n_channels = 2048\n",
    "        #else:\n",
    "            #raise NotImplementedError('model not supported: {}'.format(args.model))\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "\n",
    "        self.classifier.add_module('W1', nn.Linear(\n",
    "            n_channels, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class projection_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''Projection head for the pretraining of the resnet encoder.\n",
    "            - Uses the dataset and model size to determine encoder output\n",
    "                representation dimension.\n",
    "            - Outputs to a dimension of 128, and uses non-linear activation\n",
    "                as described in SimCLR paper: https://arxiv.org/pdf/2002.05709.pdf\n",
    "        '''\n",
    "        super(projection_MLP, self).__init__()\n",
    "\n",
    "        #if args.model == 'resnet18' or args.model == 'resnet34':\n",
    "            #n_channels = 512\n",
    "        #elif args.model == 'resnet50' or args.model == 'resnet101' or args.model == 'resnet152':\n",
    "        n_channels = 2048\n",
    "        #else:\n",
    "            #raise NotImplementedError('model not supported: {}'.format(args.model))\n",
    "\n",
    "        self.projection_head = nn.Sequential()\n",
    "\n",
    "        self.projection_head.add_module('W1', nn.Linear(\n",
    "            n_channels, n_channels))\n",
    "        self.projection_head.add_module('BN1', nn.BatchNorm1d(n_channels))\n",
    "        self.projection_head.add_module('ReLU', nn.ReLU())\n",
    "        self.projection_head.add_module('W2', nn.Linear(\n",
    "            n_channels, 128))\n",
    "        self.projection_head.add_module('BN2', nn.BatchNorm1d(128))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coco(Dataset):\n",
    "    def __init__(self, data, split='train'):\n",
    "        self.dataset = data\n",
    "        if split=='train':\n",
    "            self.img_del = set([453695, 67837, 324442, 449244, 247547, 284688, 494473, 338911, 273188, 26165, 278660, 494466, 519555, 253109, 332543, 69087, 180095, 564043, 140613, 252956, 510230, 167510, 305634, 386542, 45471, 347177, 359106, 166356, 426519, 66427, 389948, 203846, 30307, 43353, 209746, 347612, 157209, 256082, 256082, 85529, 470697, 356116, 256475, 484354, 324971, 313481, 574376, 441795, 74176, 160828, 228505, 169602, 64750, 406723, 167962, 460997, 437540, 364256, 551575, 153864, 20179, 412062, 251750, 58647, 562557, 497466, 290700, 240274, 337527, 59282, 455427, 50410, 255479, 390646, 547830, 266026, 312385, 673, 145549, 140921, 138022, 128172, 129926, 126972, 122724, 120940, 115358, 119370, 436387, 311300, 551701, 261710, 555120, 357799, 296243, 226599, 22575, 547352, 158130, 103498, 453310, 400809, 466710, 28650, 273633, 113276, 452909, 524649, 12805, 274556, 32903, 395409, 196789, 1392, 4978, 5172, 21320, 21801, 22530, 15839, 19194, 13144, 24787, 333998, 330701, 329616, 400475, 504023, 362898, 72354, 443005, 145348, 305385, 422432, 343629, 133812, 423058, 171453, 318476, 41772, 341299, 93852, 560632, 112801])   \n",
    "        else:\n",
    "            self.img_del = set([580197, 568814, 562243, 560911, 17905, 21604, 22371, 4134, 4395, 5060, 8532, 9483, 565877, 561256, 559543, 554579, 551439, 547336, 546717, 512929, 358195, 381639, 8690, 9448, 27186, 66886, 162415, 252332, 571008, 309173, 489339, 498807, 107554, 127517, 164115, 278705, 102805, 213255, 442456, 566282, 357737, 543043, 41633, 65485, 141597, 198805, 89556, 263594, 293071, 231339, 456662, 527750, 425226, 215114, 273712, 282296, 802, 28452, 57238, 168593, 187271])\n",
    "        self.coco_object2id = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, 25: 23, 27: 24, 28: 25, 31: 26, 32: 27, 33: 28, 34: 29, 35: 30, 36: 31, 37: 32, 38: 33, 39: 34, 40: 35, 41: 36, 42: 37, 43: 38, 44: 39, 46: 40, 47: 41, 48: 42, 49: 43, 50: 44, 51: 45, 52: 46, 53: 47, 54: 48, 55: 49, 56: 50, 57: 51, 58: 52, 59: 53, 60: 54, 61: 55, 62: 56, 63: 57, 64: 58, 65: 59, 67: 60, 70: 61, 72: 62, 73: 63, 74: 64, 75: 65, 76: 66, 77: 67, 78: 68, 79: 69, 80: 70, 81: 71, 82: 72, 84: 73, 85: 74, 86: 75, 87: 76, 88: 77, 89: 78, 90: 79}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
    "        image_std = torch.tensor([0.26862954, 0.26130258, 0.27577711])\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        images = self.dataset[idx][0]\n",
    "        image_input = torch.tensor(np.stack(images))\n",
    "        image_input -= image_mean[:, None, None]\n",
    "        image_input /= image_std[:, None, None]\n",
    "        \n",
    "        anns = self.dataset[idx][1]\n",
    "        labels_binary = np.zeros(80)\n",
    "        \n",
    "        cats = []\n",
    "        if len(anns)>0:\n",
    "            img_id = anns[0]['image_id']\n",
    "            if img_id in self.img_del:\n",
    "                return self.__getitem__(idx+1)\n",
    "        for ann in anns:\n",
    "            x = ann['category_id']\n",
    "            cats.append(self.coco_object2id[x])\n",
    "            \n",
    "        cats = np.array(cats)\n",
    "        assert((cats >=0).all() and (cats <= 79).all())\n",
    "        if(len(cats) ==0):\n",
    "            sample = (image_input, labels_binary)\n",
    "        else:    \n",
    "            labels_binary[cats] = 1\n",
    "            sample = (image_input, labels_binary)\n",
    "        return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-stations",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import GaussianBlur\n",
    "def load_coco(dataType, dataDir):\n",
    "    annFile='{}annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "    coco=COCO(annFile)\n",
    "    annFile_caps = '{}annotations/captions_{}.json'.format(dataDir,dataType)\n",
    "    coco_caps=COCO(annFile_caps)\n",
    "    return coco, coco_caps\n",
    "\n",
    "jitter_d = 1.0\n",
    "jitter_p = 0.8\n",
    "\n",
    "blur_sigma = [0.1, 2.0]\n",
    "blur_p = 0.5\n",
    "grey_p = 0.2\n",
    "crop_dim = 256\n",
    "\n",
    "# guassian_blur from https://github.com/facebookresearch/moco/\n",
    "guassian_blur = transforms.RandomApply([GaussianBlur(25)], p=blur_p)\n",
    "color_jitter = transforms.ColorJitter(\n",
    "    0.8*jitter_d, 0.8*jitter_d, 0.8*jitter_d, 0.2*jitter_d)\n",
    "rnd_color_jitter = transforms.RandomApply([color_jitter], p=jitter_p)\n",
    "\n",
    "rnd_grey = transforms.RandomGrayscale(p=grey_p)\n",
    "\n",
    "    \n",
    "train_preprocess = transforms.Compose([\n",
    "        Resize((256, 256), interpolation=Image.BICUBIC),\n",
    "        rnd_color_jitter,\n",
    "        rnd_grey,\n",
    "        guassian_blur,\n",
    "        transforms.RandomResizedCrop((crop_dim, crop_dim), scale=(0.008, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_preprocess = transforms.Compose([\n",
    "        transforms.CenterCrop((crop_dim * 0.875, crop_dim * 0.875)),\n",
    "        transforms.Resize((crop_dim, crop_dim)),\n",
    "        transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "'''\n",
    "train_preprocess = transforms.Compose([\n",
    "        Resize((256, 256), interpolation=Image.BICUBIC),\n",
    "        GaussianBlur(25),\n",
    "        RandomCrop(256),\n",
    "        ToTensor()\n",
    "])\n",
    "\n",
    "val_preprocess = transforms.Compose([\n",
    "        Resize((256, 256), interpolation=Image.BICUBIC),\n",
    "        GaussianBlur(25),\n",
    "        CenterCrop(256),\n",
    "        ToTensor()\n",
    "])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "train_imgs = '/localtmp/data/coco2017/coco_dataset/' + \"train2017/train2017\"\n",
    "val_imgs = '/localtmp/data/coco2017/coco_dataset/' + \"val2017\"\n",
    "train_anns = '/localtmp/data/coco2017/coco_dataset/' + \"annotations/instances_train2017.json\"\n",
    "val_anns = '/localtmp/data/coco2017/coco_dataset/' + \"annotations/instances_val2017.json\"\n",
    "coco_train = dset.CocoDetection(root=train_imgs, annFile=train_anns, \n",
    "                                transform=train_preprocess)\n",
    "coco_val = dset.CocoDetection(root=val_imgs, annFile=val_anns,\n",
    "                                transform=val_preprocess)\n",
    "\n",
    "val_dataset = Coco(coco_val, split='val')\n",
    "train_dataset = Coco(coco_train, split='train')\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                        batch_size=256,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=10)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                        batch_size=256,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=1)\n",
    "dataloaders_dict = dict()\n",
    "dataloaders_dict['train'] = train_dataloader\n",
    "dataloaders_dict['val'] = val_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-dressing",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls experimental_work/SimCLRv2-Pytorch/r50_1x_sk0.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    '''Initialize weights with zeros\n",
    "    '''\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(mean=0.0, std=0.01)\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "base_encoder = resnet50x1(num_classes=1000)\n",
    "checkpoint = torch.load('models_def/ResNet50_1x/resnet50-1x.pth')\n",
    "\n",
    "\n",
    "#checkpoint = torch.load('experimental_work/SimCLRv2-Pytorch/r50_1x_sk0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_encoder.load_state_dict(checkpoint['state_dict'])\n",
    "#base_encoder.load_state_dict(checkpoint['resnet'])\n",
    "base_encoder.fc = nn.Sequential() # ... x 1000 \n",
    "\n",
    "sup_head = Sup_Head(80)\n",
    "# 2048 x 80 \n",
    "sup_head.apply(init_weights)\n",
    "\n",
    "base_encoder.to(device)\n",
    "sup_head.to(device)\n",
    "    \n",
    "    \n",
    "'''\n",
    "# Get available models from /model/network.py\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "                     if name.islower() and not name.startswith(\"__\")\n",
    "                     and callable(models.__dict__[name]))\n",
    "# Load model\n",
    "base_encoder = getattr(models, 'resnet50')(args, num_classes=80)  # Encoder\n",
    "sup_head = models.Sup_Head(args)\n",
    "# Remove last FC layer from resnet\n",
    "base_encoder.fc = nn.Sequential()\n",
    "\n",
    "base_encoder.to(device)\n",
    "sup_head.to(device)\n",
    "\n",
    "'''\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-attendance",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-estate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 151/463 [12:59<48:25,  9.31s/it] Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 33%|███▎      | 152/463 [13:03<41:01,  7.91s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 37%|███▋      | 172/463 [14:14<19:40,  4.06s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 38%|███▊      | 174/463 [14:29<24:23,  5.07s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 39%|███▉      | 180/463 [14:32<05:21,  1.14s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 43%|████▎     | 200/463 [15:43<04:49,  1.10s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [35:37<00:00,  4.62s/it]\n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.043918348002230226\n",
      "EPOCH LOSS:  0.14215280325215227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 121/463 [06:53<05:21,  1.06it/s] Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 27%|██▋       | 127/463 [07:26<11:39,  2.08s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 30%|███       | 141/463 [08:03<05:07,  1.05it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 32%|███▏      | 148/463 [08:35<08:27,  1.61s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 33%|███▎      | 151/463 [08:37<04:55,  1.06it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 35%|███▍      | 161/463 [09:13<04:52,  1.03it/s]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [28:34<00:00,  3.70s/it]\n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.04446233323982118\n",
      "EPOCH LOSS:  0.1416472860604392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 137/463 [10:37<17:10,  3.16s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 30%|███       | 140/463 [10:39<08:12,  1.52s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 30%|███       | 141/463 [10:56<34:08,  6.36s/it]ingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 31%|███       | 142/463 [10:57<25:31,  4.77s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>><bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "        self._shutdown_workers()self._shutdown_workers()\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)    \n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'AssertionError\n",
      ": AssertionErrorcan only join a child process: \n",
      "can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 37%|███▋      | 170/463 [13:08<07:04,  1.45s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [35:46<00:00,  4.64s/it]\n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.045406262793743137\n",
      "EPOCH LOSS:  0.13991810775436112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 140/463 [10:35<05:07,  1.05it/s]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    Exception ignored in: w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'    \n",
      "self._shutdown_workers()\n",
      "AssertionError  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    : can only join a child process\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 35%|███▍      | 160/463 [12:09<06:05,  1.21s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 35%|███▍      | 161/463 [12:53<1:09:41, 13.85s/it]Traceback (most recent call last):\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      " 35%|███▍      | 162/463 [12:53<49:35,  9.88s/it]      assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 36%|███▌      | 165/463 [12:55<19:08,  3.85s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 37%|███▋      | 170/463 [12:59<05:55,  1.21s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [36:01<00:00,  4.67s/it]  \n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.04609265654941144\n",
      "EPOCH LOSS:  0.13840884491672342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 40/463 [03:33<10:33,  1.50s/it]  Exception ignored in: Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>><bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Traceback (most recent call last):\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "self._shutdown_workers()\n",
      "    self._shutdown_workers()  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    \n",
      "Exception ignored in: w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "          File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    Exception ignored in: self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only join a child process'<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "\n",
      "AssertionError\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "\n",
      ":   File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "can only join a child process  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "        self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "AssertionError    : assert self._parent_pid == os.getpid(), 'can only join a child process'    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "AssertionError: can only join a child processcan only join a child process\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 13%|█▎        | 60/463 [05:11<09:45,  1.45s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "        assert self._parent_pid == os.getpid(), 'can only join a child process'self._shutdown_workers()\n",
      "\n",
      "AssertionError  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      ":     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)can only join a child process\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 14%|█▎        | 63/463 [05:50<42:18,  6.35s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [40:16<00:00,  5.22s/it]  \n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.046544591891693375\n",
      "EPOCH LOSS:  0.13698915259805608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 40/463 [03:29<08:55,  1.27s/it]  Exception ignored in: Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>Exception ignored in: Traceback (most recent call last):\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>Exception ignored in: Exception ignored in:     <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>    \n",
      "self._shutdown_workers()Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "self._shutdown_workers()\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    Traceback (most recent call last):\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "self._shutdown_workers()\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "    self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only join a child process'        self._shutdown_workers()\n",
      "\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "Exception ignored in:     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'AssertionError\n",
      "\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    : AssertionErrorException ignored in: : self._shutdown_workers()    \n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'can only join a child processTraceback (most recent call last):\n",
      "\n",
      "can only join a child process\n",
      "\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'assert self._parent_pid == os.getpid(), 'can only join a child process'      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "\n",
      "AssertionError    \n",
      ": AssertionErrorcan only join a child processassert self._parent_pid == os.getpid(), 'can only join a child process'AssertionError\n",
      "self._shutdown_workers(): : \n",
      "can only join a child processcan only join a child process  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "AssertionError\n",
      "\n",
      ": \n",
      "    can only join a child processw.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only join a child process'<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "AssertionError\n",
      ": Traceback (most recent call last):\n",
      "can only join a child process  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    \n",
      "self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 13%|█▎        | 60/463 [05:12<10:31,  1.57s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [35:54<00:00,  4.65s/it] \n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.04696426245670475\n",
      "EPOCH LOSS:  0.1363357461945982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 53/463 [04:59<46:01,  6.74s/it]  Exception ignored in: Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    Exception ignored in: w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "self._shutdown_workers()  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "    \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'        w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "self._shutdown_workers()\n",
      "Exception ignored in: \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    AssertionError: assert self._parent_pid == os.getpid(), 'can only join a child process'Exception ignored in:   File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "can only join a child process<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "\n",
      "\n",
      "AssertionError<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Traceback (most recent call last):\n",
      "    : \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "can only join a child process\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "AssertionErrorself._shutdown_workers()        self._shutdown_workers()assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    :     w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)can only join a child process\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: AssertionErrorcan only join a child process: \n",
      "can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      " 12%|█▏        | 54/463 [05:08<50:37,  7.43s/it]    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 12%|█▏        | 55/463 [05:08<36:48,  5.41s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 12%|█▏        | 57/463 [05:10<20:17,  3.00s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 13%|█▎        | 60/463 [05:12<09:52,  1.47s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [36:08<00:00,  4.68s/it] \n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.04751618862108803\n",
      "EPOCH LOSS:  0.13597644802888123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 30/463 [02:36<08:23,  1.16s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    Exception ignored in: w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)<bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    \n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'    \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "self._shutdown_workers()AssertionError:     \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)can only join a child process\n",
      "\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "AssertionError: can only join a child process    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 11%|█         | 50/463 [04:24<10:51,  1.58s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Exception ignored in:     <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "\n",
      "AssertionErrorTraceback (most recent call last):\n",
      ":   File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "can only join a child process    self._shutdown_workers()\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 11%|█         | 51/463 [04:56<1:13:00, 10.63s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "100%|██████████| 463/463 [36:07<00:00,  4.68s/it]  \n",
      "  0%|          | 0/463 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEAN AVG PRECISION:  0.04802493492678561\n",
      "EPOCH LOSS:  0.13556873587672877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 23/463 [02:35<54:42,  7.46s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "Exception ignored in:     <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "self._shutdown_workers()\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "self._shutdown_workers()\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "\n",
      "      File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "assert self._parent_pid == os.getpid(), 'can only join a child process'        \n",
      "AssertionErrorassert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "self._shutdown_workers()AssertionError\n",
      ": can only join a child process: \n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "can only join a child process    \n",
      "  5%|▌         | 24/463 [02:36<39:53,  5.45s/it]\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "  5%|▌         | 25/463 [02:36<29:36,  4.06s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "  6%|▌         | 27/463 [02:39<19:20,  2.66s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 10%|█         | 47/463 [04:18<15:22,  2.22s/it]  Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      " 10%|█         | 48/463 [04:22<19:30,  2.82s/it]assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 11%|█         | 50/463 [04:27<19:25,  2.82s/it]Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f643c9474e0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/u/lab/jr4fs/anaconda2/envs/coco_exp/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      " 78%|███████▊  | 360/463 [28:46<01:34,  1.09it/s]  "
     ]
    }
   ],
   "source": [
    "\n",
    "''' Finetuning / Evaluate '''\n",
    "\n",
    "# Do not Pretrain, just finetune and inference\n",
    "\n",
    "#print(\"\\n\\nLoading the model: {}\\n\\n\".format(args.load_checkpoint_dir))\n",
    "\n",
    "# Load the pretrained model\n",
    "#checkpoint = torch.load(args.load_checkpoint_dir)\n",
    "\n",
    "# Load the encoder parameters\n",
    "#base_encoder.load_state_dict(checkpoint['encoder'])  # .cuda()\n",
    "\n",
    "# Initalize weights of the supervised / classification head\n",
    "#sup_head.apply(init_weights)\n",
    "\n",
    "# Supervised Finetuning of the supervised classification head\n",
    "finetune(base_encoder, sup_head, dataloaders_dict)\n",
    "\n",
    "# Evaluate the pretrained model and trained supervised head\n",
    "#test_loss, test_acc, test_acc_top5 = evaluate(\n",
    "    #base_encoder, sup_head, dataloaders_dict, 'val', 10)\n",
    "\n",
    "#print('[Test] loss {:.4f} - acc {:.4f} - acc_top5 {:.4f}'.format(\n",
    "    #test_loss, test_acc, test_acc_top5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-holocaust",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
